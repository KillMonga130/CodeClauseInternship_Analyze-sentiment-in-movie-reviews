{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354f2f43-6f9c-44fb-8e6d-a0f842852684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8626e97b-8a94-44a6-a5b9-8440f67f83a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "                                                text  label\n",
      "0  I grew up (b. 1965) watching and loving the Th...      0\n",
      "1  When I put this movie in my DVD player, and sa...      0\n",
      "2  Why do people who do not know what a particula...      0\n",
      "3  Even though I have great interest in Biblical ...      0\n",
      "4  Im a die hard Dads Army fan and nothing will e...      1\n",
      "\n",
      "Testing Data:\n",
      "                                                text  label\n",
      "0  I always wrote this series off as being a comp...      0\n",
      "1  1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
      "2  This movie was so poorly written and directed ...      0\n",
      "3  The most interesting thing about Miryang (Secr...      1\n",
      "4  when i first read about \"berlin am meer\" i did...      0\n",
      "\n",
      "Validation Data:\n",
      "                                                text  label\n",
      "0  It's been about 14 years since Sharon Stone aw...      0\n",
      "1  someone needed to make a car payment... this i...      0\n",
      "2  The Guidelines state that a comment must conta...      0\n",
      "3  This movie is a muddled mish-mash of clich√©s f...      0\n",
      "4  Before Stan Laurel became the smaller half of ...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Load testing data\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Load validation data\n",
    "valid_df = pd.read_csv(\"valid.csv\")\n",
    "\n",
    "# Display the first few rows of each dataset to verify they're loaded correctly\n",
    "print(\"Training Data:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTesting Data:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\nValidation Data:\")\n",
    "print(valid_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c901ad9e-f1ce-4c67-b105-6a2224612b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Label Distribution:\n",
      "label\n",
      "0    20019\n",
      "1    19981\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing Data Label Distribution:\n",
      "label\n",
      "1    2505\n",
      "0    2495\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation Data Label Distribution:\n",
      "label\n",
      "1    2514\n",
      "0    2486\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Label Distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print(\"\\nTesting Data Label Distribution:\")\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "print(\"\\nValidation Data Label Distribution:\")\n",
    "print(valid_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd56a15d-dc84-47c7-bfe4-fb931582d996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Review Length Distribution:\n",
      "count    40000.000000\n",
      "mean      1310.293250\n",
      "std        988.358599\n",
      "min         32.000000\n",
      "25%        698.000000\n",
      "50%        973.000000\n",
      "75%       1596.000000\n",
      "max      13704.000000\n",
      "Name: review_length, dtype: float64\n",
      "\n",
      "Testing Data Review Length Distribution:\n",
      "count     5000.000000\n",
      "mean      1314.596200\n",
      "std       1010.339949\n",
      "min         67.000000\n",
      "25%        706.000000\n",
      "50%        970.000000\n",
      "75%       1578.500000\n",
      "max      12930.000000\n",
      "Name: review_length, dtype: float64\n",
      "\n",
      "Validation Data Review Length Distribution:\n",
      "count    5000.00000\n",
      "mean     1297.36800\n",
      "std       979.91039\n",
      "min        52.00000\n",
      "25%       698.00000\n",
      "50%       957.00000\n",
      "75%      1560.25000\n",
      "max      9345.00000\n",
      "Name: review_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_df['review_length'] = train_df['text'].apply(len)\n",
    "test_df['review_length'] = test_df['text'].apply(len)\n",
    "valid_df['review_length'] = valid_df['text'].apply(len)\n",
    "\n",
    "print(\"Training Data Review Length Distribution:\")\n",
    "print(train_df['review_length'].describe())\n",
    "\n",
    "print(\"\\nTesting Data Review Length Distribution:\")\n",
    "print(test_df['review_length'].describe())\n",
    "\n",
    "print(\"\\nValidation Data Review Length Distribution:\")\n",
    "print(valid_df['review_length'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02269c0-e03e-425b-ba74-f0521b696192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mubva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mubva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mubva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8934\n",
      "Validation Accuracy: 0.8918\n",
      "\n",
      "Classification Report for Testing Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      2495\n",
      "           1       0.88      0.91      0.89      2505\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.89      2486\n",
      "           1       0.88      0.91      0.89      2514\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Perform stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    # Perform lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Load your training, testing, and validation datasets\n",
    "# Replace 'path_to_your_datasets' with the actual path to your datasets\n",
    "train_df = pd.read_csv(\"Train.csv\")\n",
    "test_df = pd.read_csv(\"Test.csv\")\n",
    "valid_df = pd.read_csv(\"Valid.csv\")\n",
    "\n",
    "# Apply preprocessing to each review in the datasets\n",
    "train_df['preprocessed_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['preprocessed_text'] = test_df['text'].apply(preprocess_text)\n",
    "valid_df['preprocessed_text'] = valid_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit-transform the training data and transform the testing and validation data\n",
    "X_train = tfidf_vectorizer.fit_transform(train_df['preprocessed_text'])\n",
    "X_test = tfidf_vectorizer.transform(test_df['preprocessed_text'])\n",
    "X_valid = tfidf_vectorizer.transform(valid_df['preprocessed_text'])\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "y_valid = valid_df['label']\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on testing and validation data\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "y_pred_valid = logreg.predict(X_valid)\n",
    "\n",
    "# Evaluate performance\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "valid_accuracy = accuracy_score(y_valid, y_pred_valid)\n",
    "\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "print(\"Validation Accuracy:\", valid_accuracy)\n",
    "\n",
    "print(\"\\nClassification Report for Testing Data:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\nClassification Report for Validation Data:\")\n",
    "print(classification_report(y_valid, y_pred_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60f6c4-48dc-4676-8d3c-36a4f04e1e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
